{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b621a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DistilBertForSequenceClassification, file_utils, Pipeline, AutoTokenizer, pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5691b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./results/models/best\"\n",
    "model_describ = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c773ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model2 = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_describ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1564413",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"admiration\",\n",
    "    \"amusement\",\n",
    "    \"anger\",\n",
    "    \"annoyance\",\n",
    "    \"approval\",\n",
    "    \"caring\",\n",
    "    \"confusion\",\n",
    "    \"curiosity\",\n",
    "    \"desire\",\n",
    "    \"disappointment\",\n",
    "    \"disapproval\",\n",
    "    \"disgust\",\n",
    "    \"embarrassment\",\n",
    "    \"excitement\",\n",
    "    \"fear\",\n",
    "    \"gratitude\",\n",
    "    \"grief\",\n",
    "    \"joy\",\n",
    "    \"love\",\n",
    "    \"nervousness\",\n",
    "    \"optimism\",\n",
    "    \"pride\",\n",
    "    \"realization\",\n",
    "    \"relief\",\n",
    "    \"remorse\",\n",
    "    \"sadness\",\n",
    "    \"surprise\",\n",
    "    \"neutral\"\n",
    "  ]\n",
    "id2label = {i:label for i,label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is pretty much the source code from TextClassificationPipeline, but when i subclassed it\n",
    "# didnt correctly work(not sure why, didn't try again at the end) so i just copied the whole code.\n",
    "# We cant use TextClassification itself, since it only outputs the highest label\n",
    "if file_utils.is_tf_available():\n",
    "    from transformers.models.auto.modeling_tf_auto import TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
    "\n",
    "if file_utils.is_torch_available():\n",
    "    from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
    "\n",
    "class MultiLabelTextClassification(Pipeline):\n",
    "    \"\"\"\n",
    "    Text classification pipeline using any :obj:`ModelForSequenceClassification`. See the `sequence classification\n",
    "    examples <../task_summary.html#sequence-classification>`__ for more information.\n",
    "\n",
    "    This text classification pipeline can currently be loaded from :func:`~transformers.pipeline` using the following\n",
    "    task identifier: :obj:`\"sentiment-analysis\"` (for classifying sequences according to positive or negative\n",
    "    sentiments).\n",
    "\n",
    "    If multiple classification labels are available (:obj:`model.config.num_labels >= 2`), the pipeline will run a\n",
    "    softmax over the results. If there is a single label, the pipeline will run a sigmoid over the result.\n",
    "\n",
    "    The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\n",
    "    the up-to-date list of available models on `huggingface.co/models\n",
    "    <https://huggingface.co/models?filter=text-classification>`__.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_all_scores: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.check_model_type(\n",
    "            TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
    "            if self.framework == \"tf\"\n",
    "            else MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
    "        )\n",
    "\n",
    "        self.return_all_scores = return_all_scores\n",
    "\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Classify the text(s) given as inputs.\n",
    "\n",
    "        Args:\n",
    "            args (:obj:`str` or :obj:`List[str]`):\n",
    "                One or several texts (or one list of prompts) to classify.\n",
    "\n",
    "        Return:\n",
    "            A list or a list of list of :obj:`dict`: Each result comes as list of dictionaries with the following keys:\n",
    "\n",
    "            - **label** (:obj:`str`) -- The label predicted.\n",
    "            - **score** (:obj:`float`) -- The corresponding probability.\n",
    "\n",
    "            If ``self.return_all_scores=True``, one such dictionary is returned per label.\n",
    "        \"\"\"\n",
    "        outputs = super().__call__(*args, **kwargs)\n",
    "\n",
    "\n",
    "        scores = np.exp(outputs) / (1+np.exp(outputs))\n",
    "        if self.return_all_scores:\n",
    "            return [\n",
    "                [{\"label\": self.model.config.id2label[i], \"score\": score.item()} for i, score in enumerate(item)]\n",
    "                for item in scores\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                {\"label\": self.model.config.id2label[item.argmax()], \"score\": item.max().item()} for item in scores\n",
    "            ]\n",
    "\n",
    "def analyze_result(result, threshold = 0.5):\n",
    "    \"\"\"Sort the results and throw away all labels with prediction under threshold\"\"\"\n",
    "    output = []\n",
    "    for sample in result:\n",
    "        sample = np.array(sample)\n",
    "        scores = np.array([label['score'] for label in sample])\n",
    "        predicted_samples = np.argwhere(scores > threshold).reshape(-1)\n",
    "        output.append(sorted(sample[predicted_samples], key = lambda item: item['score'], reverse=True))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    \"return_all_scores\":True,\n",
    "    \"device\":0    \n",
    "}\n",
    "inference_pipeline = MultiLabelTextClassification(model=model2, tokenizer=tokenizer, **pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_classifier = \"typeform/distilbert-base-uncased-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pipeline(\"zero-shot-classification\", device=0, model=zero_shot_classifier, tokenizer = zero_shot_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731f8d5",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/friends-final-raw.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d80c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"tennis-corpus\"))\n",
    "utterances = list(corpus.iter_utterances())\n",
    "texts = [t.text for t in utterances]\n",
    "speakers = [t.get_speaker().id for t in utterances]\n",
    "dataset = pd.DataFrame(zip(texts, speakers), columns=['line', 'person']) # todo: change name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaca29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into sentences\n",
    "dataset[\"line\"] = dataset[\"line\"].str.split(r'[.!?]+\\s')\n",
    "person_counts = dataset[\"person\"].value_counts()\n",
    "main_characters = person_counts[person_counts > 1000].reset_index()[\"index\"]\n",
    "print(main_characters)\n",
    "\n",
    "dataset = dataset[dataset[\"person\"].isin(main_characters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_emotions = {}\n",
    "for p in main_characters:\n",
    "    predicted_emotions[p] = list()\n",
    "\n",
    "for row in tqdm(dataset.itertuples(), total=len(dataset)):\n",
    "    prediction = temp(row.line, labels, multi_label=True)\n",
    "    if len(row.line) == 1:\n",
    "        prediction = [prediction]\n",
    "    #x = inference_pipeline(row.line)\n",
    "    prediction = [[{'label' : label, 'score': value} for label, value in zip(sentence['labels'], sentence['scores'])] for sentence in prediction]\n",
    "    result = analyze_result(prediction, .8)\n",
    "    result = [(pred['label'], pred['score']) for pred in result[0]] \n",
    "    predicted_emotions[row.person].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878500aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_emotions_per_person = {}\n",
    "for p in main_characters:\n",
    "    total_emotions_per_person[p] = {}\n",
    "    for l in labels:\n",
    "        total_emotions_per_person[p][l] = 0\n",
    "\n",
    "for person, sentences in predicted_emotions.items():    \n",
    "    for s in sentences:\n",
    "        for e in s:\n",
    "            total_emotions_per_person[person][e[0]] += (e[1]/ len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d473a5",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09844b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for person, emotions in total_emotions_per_person.items():\n",
    "    plt.title(person)\n",
    "    plt.ylim((0,0.1))\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.bar(range(0, len(labels)), emotions.values(), tick_label=labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"person\", \"emotion\", \"value\"])\n",
    "\n",
    "for person, emotions in total_emotions_per_person.items():\n",
    "    for emotion, value in emotions.items():\n",
    "        df.loc[len(df)] = [person,emotion,value]\n",
    "        \n",
    "df = df[df[\"emotion\"] != \"neutral\"]\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "chart = sns.barplot(x=\"emotion\", y=\"value\", hue=\"person\", data=df)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4edf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
